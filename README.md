<!-- Website -->

<p><a href="https://www.linkedin.com/in/jaume-clave-domenech/" target="_blank">
<img alt="GitHub" src="https://jaumeclave.github.io./images/linkedin_logo.png" height="17"></a>
<a href="https://github.com/JaumeClave" target="_blank">
<img alt="GitHub" src="https://jaumeclave.github.io./images/github_logo.png" height="18" hspace="20"></a>
<a href="mailto:j.clavedomenech@gmail.com" target="_blank">
<img alt="Gmail" src="https://jaumeclave.github.io./images/gmail_logo.png" height="18"></a></p>
<p><a href="https://github.com/JaumeClave/JaumeClave.github.io/raw/master/curriculum_vitae/CV_Jaume_Clave_Domenech.pdf" download>
<b>Curriculum (CV)</b></a></p>

<img src="https://jaumeclave.github.io./images/jaume_clave.jpg" width="200" align="right"/>

# Data Science Portfolio by Jaume Clave Domenech
<p align="justify">This portfolio is a compilation of notebooks which I created for data analysis and for exploration of machine learning algorithms. It contains work from projects I have researched out of interest and curiosity, my professional career as a Data Scientist in the FinTech and SaaS industries and from my time completing my MSc Business Analytics at Imperial College Business School.</p>

<p><a href="https://jaumeclave.github.io./#about">About </a>
<a href="https://jaumeclave.github.io./#certifications">Certifications </a>
<a href="https://jaumeclave.github.io./#hackathons">Hackathons </a>
<a href="https://jaumeclave.github.io./#papers">Papers </a>
<a href="https://jaumeclave.github.io./#projects">Projects </a></p>

## Projects
<h3 id="yolov4-custom-object-detector">A Deep Learning Approach to Identifying Messi: Darknet YOLOv4 Custom Object Detector</h3>
<p><a href="https://colab.research.google.com/github/JaumeClave/darknet_volov4_object_detection/blob/master/Darknet_YOLOv4_Object_Detection.ipynb" target="_blank">Google Colab</a></p>

<a href="https://colab.research.google.com/github/JaumeClave/darknet_volov4_object_detection/blob/master/Darknet_YOLOv4_Object_Detection.ipynb" target="_blank">
<img src="https://jaumeclave.github.io./images/messi_atletic.gif" width="350" align="right"/></a>

<p align="justify">Object detection is a computer vision technology that deals with detecting instances of semantic objects of certain classes, such as humans, cars, weapons, etc…, in digital images and videos. The use cases of object detection models are endless as they help in tasks relating to image annotation, activity recognition, face detection, face recognition, video object co-segmentation and the tracking of objects. Advanced militaries use object detection through the combination of human visual salience and visual psychology, so as to achieve rapid and accurate detection of military objects on the vast and complex battlefield. Smart cities rely on the use of object detection to implement smart traffic management systems able to detect vacant on-street parking. <a href="https://colab.research.google.com/github/JaumeClave/darknet_volov4_object_detection/blob/master/Darknet_YOLOv4_Object_Detection.ipynb" target="_blank"><b>This project</b></a> trains a Darknet framework YOLOv4 object detection model to identify and follow Messi throughout a football game. Messi is the greatest footballer to ever play and how he plays defines his greatness, his patience and off ball movement consistently get him into scoring opportunities which he rarely wastes. When he has the ball, his low center of gravity and reactions allow him to move into space where their previously was none. His quickness and agility along with the complex task of identifying only him in a situation where there are 22 other players, 11 kitted the same, made this a great project to learn about object detection techniques. 
</p>


<h3 id="deepfakes-first-order-model">Computer Vision Deepfakes (First Order Model Method): Face Synthesis With GANs and Autoencoders</h3>
<p><a href="https://colab.research.google.com/github/JaumeClave/deepfakes_first_order_model/blob/master/first_order_model_deepfakes.ipynb" target="_blank">Google Colab</a></p>

<a href="https://colab.research.google.com/github/JaumeClave/deepfakes_first_order_model/blob/master/first_order_model_deepfakes.ipynb" target="_blank">
<img src="https://jaumeclave.github.io./images/collage_trim_trimmed.gif" width="350" align="right"/></a>

<p align="justify">The term deepfakes originated in 2017 when a Reddit user named “deepfakes” shared the first ever of this media online. These videos involved celebrities’ faces swapped onto the bodies of actresses in pornographic videos. Since then academic research and interest in deepfakes (a portmanteau of "deep learning" and "fake") has grown quickly. Deepfakes leverage powerful machine learning to manipulate a source image and generate a video using a driving media to generate visual content with a high potential to deceive. Deepfakes have garnered widespread attention for their uses in celebrity pornographic videos, fake news, hoaxes, and financial fraud. This has elicited responses from both industry and government to detect and limit their use. <a href="https://colab.research.google.com/github/JaumeClave/deepfakes_first_order_model/blob/master/first_order_model_deepfakes.ipynb" target="_blank"><b>This project</b></a> uses Aliaksandr Siarohin First Order Model Method to generate and create deepfakes of various politicians and famous sports figures. The project explains deepfakes and what they are. How they can be created and the technology behind them including generative adversarial networks, neural networks and autoencoders. The paper ends with a section about the ethics of these media and the importance of recognizing that they exist so that the public is less likely to be deceived. 
</p>


<h3 id="whatsapp_chat_anlaysis">WhatsApp Group Chat Analysis With Naive Bayes Message Categorization</h3>
<p><a href="https://nbviewer.jupyter.org/github/JaumeClave/whatsapp_group_chat_analysis/blob/master/whatsapp_expansion_crew_final.ipynb" target="_blank">nbviewer</a></p>

<a href="https://nbviewer.jupyter.org/github/JaumeClave/whatsapp_group_chat_analysis/blob/master/whatsapp_expansion_crew_final.ipynb" target="_blank">
<img src="https://jaumeclave.github.io./images/chat_polarity_scores.JPG" width="350" align="right"/></a>

<p align="justify">Facebook's purchase of the messaging giant for $16 billion ($4 billion cash, $12 billion in Facebook shares) back in 2014, further solidified Facebooks reach into user's everyday lives. The acquisition was one of the largest Silicon Valley had ever seen, and by far Facebook's largest purchase. So why did Facebook purchase an app that had limited revenue generation...? Growth potential and data. WhatsApp was the only app with higher engagement than Facebook itself. <a href="https://nbviewer.jupyter.org/github/JaumeClave/whatsapp_group_chat_analysis/blob/master/whatsapp_expansion_crew_final.ipynb" target="_blank"><b>This project</b></a> uses Python to explore the dataset and compute interesting top-level metrics. The paper aims to inform the reader about exploratory data analysis (EDA) methods and what they can show in relation to this type of data. The paper contains a detailed section on forecasting models including STL decomposition and ARIMA techniques that are utilised to forecast future message counts. The project studies an introduction into machine learning through natural language processing and the use of a Naive Bayes classifier for message segmentation. The visualizations go hand in hand with the explanations so as to provide a visual medium for the information. Applications and uses of proper EDA techniques and processes are explained. The importance of chat and text data to WhatsApp and a user interested in it is shown and the opportunities WhatsApp has available to it with everyones chat data are discussed.</p>


<h3 id="spotify-music-analysis">Using Spotipy, K-Means Clustering and Plotly to Analyse My Personal Spotify Music Library</h3>
<p><a href="https://nbviewer.jupyter.org/github/JaumeClave/spotipy_kmeans_plotly_music_taste_analysis/blob/4db117e8e09030c21dfde57e05c2197df7ba7be6/data_driven_study_on_my_music_taste.ipynb" target="_blank">nbviewer</a></p>

<a href="https://nbviewer.jupyter.org/github/JaumeClave/spotipy_kmeans_plotly_music_taste_analysis/blob/4db117e8e09030c21dfde57e05c2197df7ba7be6/data_driven_study_on_my_music_taste.ipynb" target="_blank">
<img src="https://jaumeclave.github.io./images/3d_song_clusters_gif.gif" width="350" align="right"/></a>

<p align="justify">Spotify, the largest on-demand music service in the world, has a history of pushing technological boundaries and using big data, artificial intelligence and machine learning to drive success. Although it's offering is music, Spotify is a data-driven company and it uses the data in every part of the organization to drive decisions. It proudly offers over 50 million ad-free tracks to its 130 million Spotify Premium subscribers for £9.99 a month, with every stream learning more about their users. The company now has a market cap of $35 billion and from 2014 to late 2019, Spotify spent hundreds of millions of dollars acquiring data science consulting firms, music intelligence agencies, personalization and recommendation feature technology, and an artificial intelligence to strengthen their data centric offering. As the service continues to acquire data points, it’s using that information to train the algorithms and machines to listen to music and extrapolate insights that impact its business and the experience of listeners. <a href="https://nbviewer.jupyter.org/github/JaumeClave/spotipy_kmeans_plotly_music_taste_analysis/blob/4db117e8e09030c21dfde57e05c2197df7ba7be6/data_driven_study_on_my_music_taste.ipynb" target="_blank"><b>This project</b></a> uses Spotipy, Spotify’s API, to access my music library and analyze trends in listening behavior over time. It takes a detailed look into how a tracks audio features impact what I listen to and when I listen to it and provides some findings in regards to my favorite artists, albums and songs. The project utilizes PCA in order to reduce the dimensions of the audio feature list before running a K-Means model in order to classify and create custom genres from my most played tracks. It also compares my personal music taste to the Global Top 50 Spotify playlist in order to compare my likings to a viral playlist.</p>


<h3 id="ocr-selenium-automation">Winning a Tesla (Hopefully!) With Selenium Automation and Optical Character Recognition</h3>
<p><a href="https://nbviewer.jupyter.org/github/JaumeClave/OCR_yeo_valley_competition/blob/master/yeo_promotional_code_ocr_automation.ipynb" target="_blank">nbviewer</a></p>

<a href="https://nbviewer.jupyter.org/github/JaumeClave/OCR_yeo_valley_competition/blob/master/yeo_promotional_code_ocr_automation.ipynb" target="_blank">
<img src="https://jaumeclave.github.io./images/yeo_entry_trim.gif" width="350" align="right"/>
</a>

<p align="justify">Optical Character Recognition (or OCR) is the conversion of images typed, handwritten or printed text into machine-encoded text. OCR can  help digitalize printed text so that it can be electronically searched for, stored and displayed. It is commonly used in machine processes such as cognitive computing, machine translation, text-to-speech and text mining and is an important field of study in pattern recognition, artificial intelligent and computer vision. Yeo Valley, a British family-owned farming and dairy company sell a fantastic brand of yogurt and their business model provides their consumers with the ability to redeem Yeokens (points) for experiences and prizes. The Yeokens are claimed by entering a dot matrix font code on the inside of their yogurt lids. An OCR engine can be linked with browser automation functions to automatize this boring stuff. <a href="https://nbviewer.jupyter.org/github/JaumeClave/OCR_yeo_valley_competition/blob/master/yeo_promotional_code_ocr_automation.ipynb" target="_blank"><b>This project</b></a> automates the process involved in entering a promotional code on a website in order to collect points and to enter in a draw to win a Tesla Model 3. The process is split into four defined tasks; image download, image preprocessing, optical character recognition of image and promotional code submission. Selenium is used to automate the first and last stages of the process, firstly logging in to WhatsApp Web and finally simulating human-like typing upon form submission to mimic human writing behavior. Steps two and three use computer vision techniques and transformations to ensure the image input for the OCR engine is optimized. All the steps are linked together to form a script which ultimately takes an image of a yogurt lid, detects a 14 character code and enters that code into an online draw.</p>


<h3 id="natural-langauge-processing-of-app-reviews">Natural Language Processing, Co-Occurance and Sentiment Analysis of App Store Reviews</h3>
<p><a href="https://nbviewer.jupyter.org/github/JaumeClave/app_review_sentiment_analysis/blob/master/app_reviews.ipynb" target="_blank">nbviewer</a></p>

<a href="https://nbviewer.jupyter.org/github/JaumeClave/app_review_sentiment_analysis/blob/master/app_reviews.ipynb" target="_blank">
<img src="https://jaumeclave.github.io./images/app_review_gif.gif" width="350" align="right"/>
</a>

<p align="justify">The mobile app as we know it is a little over 12 years old. There were 500 apps on the first iteration of Apple’s App Store in 2008. We’ve come a long way since then. The iOS App Store and Google Play offer users a combined 5 million apps and the number of downloads per year surpasses the 220 billion mark. Consumers averaged 3 hours and 40 minutes on mobile in 2019, up 35% since 2017—companies from every vertical are benefitting by making mobile the center of their digital transformation investments. Brands continue to embrace the unprecedented reach and value of mobile. Advertisers will pour more than $240 billion into ad spend in 2020—up 26% from 2019. This massive market that will only continue to grow as time and new technologies such as VR and AR become household obsessions also involves a massive community of customers who express their feelings for the apps they use. Venting about problems and frustration with the platform or expressing content about the ease of use. <a href="https://nbviewer.jupyter.org/github/JaumeClave/app_review_sentiment_analysis/blob/master/app_reviews.ipynb" target="_blank"><b>This project</b></a> scrapes the Google Play and iOS App store before conducting a sentiment analysis on app reviews for 'Asos' using the VADER sentiment library. Sentiment and polarity are tracked over fiscal quarter to gain an appreciation of how users feel for the app throughout time and to appreciate rating and subjectivity changes for different app versions. A bigram model is used to investigate and plot the most probable co-occurrences for different reviews and the result is visualised using NetworkX. Finally, an analysis on Asos competitors is conducted to create to benchmark for app performance and customer sentiment in the fast-fashion industry.</p>


<h3 id="visualising-google-trends-data-python">Coding a Python Module to Analyze, Visualize and Track Google Trends Data</h3>
<p><a href="https://nbviewer.jupyter.org/github/JaumeClave/analysing_search_google_trends/blob/master/pytrends_with_google_trends.ipynb" target="_blank">nbviewer</a></p>

<a href="https://nbviewer.jupyter.org/github/JaumeClave/analysing_search_google_trends/blob/master/pytrends_with_google_trends.ipynb" target="_blank">
<img src="https://jaumeclave.github.io./images/miami_heat_map_gif.gif" width="350" align="right"/>
</a>

<p align="justify">Launched in 2006, Google Trends, provides users with invaluable insights into how people search on the world’s most popular search engine. The vast amount of searches — trillions take place every year — make Google Trends one of the world’s largest real time datasets. Examining what people search for provides a unique perspective on what they are currently interested in and curious about. Researchers have used Google Trends data to investigate a number of questions, from exploring the course of influenza outbreaks to forecasting economic indicators. The kinds of searches that users perform can be a good proxy for the public’s interests, concerns or intentions, but these searches do not necessarily represent users’ opinions and therefore the data can be used to track industry/market traction.  <a href="https://nbviewer.jupyter.org/github/JaumeClave/analysing_search_google_trends/blob/master/pytrends_with_google_trends.ipynb" target="_blank"><b>This project</b></a> leverages the data that can be found on Google Trends through a Python library called pytrends to explore, document and visualise search interest in specific places over a set time frame. Various visualisation techniques are used to create global and country specific choropleth maps, interactive time series line graphs and animated heat maps to display search term trend patterns. Finally, through object-oriented programming, the code is packaged together to form a module that can be utilized by anyone interested in this project.</p>


<h3 id="data-science-interview-questions">Automating the Boring Stuff: Sending Myself Data Science Interview Questions</h3>
<p><a href="https://nbviewer.jupyter.org/github/JaumeClave/automating_data_science_interview_qs/blob/master/fetching_ds_qs.ipynb" target="_blank">nbviewer</a></p>

<a href="https://nbviewer.jupyter.org/github/JaumeClave/automating_data_science_interview_qs/blob/master/fetching_ds_qs.ipynb" target="_blank">
<img src="https://jaumeclave.github.io./images/data_science_int_qs_process.gif" width="350" align="right"/>
</a>

<p align="justify">Data science, also known as data-driven decision, is an interdisciplinary field about scientific methods, process and systems to extract knowledge from data in various forms, and take decision based on this knowledge. There are a lot of things that a data scientist should know in terms of programming, theory and communication. The interview process is long and difficult with various interviews and theoretical and practical exams. While I search for a permanent role as a Data Scientist I thought it would be best to spend time each day practicing and learning about what questions may be asked in the interviews so that I am prepared when the time comes. <a href="https://nbviewer.jupyter.org/github/JaumeClave/automating_data_science_interview_qs/blob/master/fetching_ds_qs.ipynb" target="_blank"><b>This project</b></a>  scrapes the web for data science interview questions and processes them before importing them into a PostgreSQL database. The database is used to query specific questions based on subjects (machine learning, statistics, programming) and a Python script is created which returns a random question and answer pairing and emails them to my inbox. This task if further automated using Windows Task Scheduler as the script is executed two times a day in order to get good interview practice. </p>


<h3 id="efficient-frontier-analysis-optimisation">Risk Management Scenario Analysis and Markowitz Efficient Frontier Portfolio Optimisation</h3>
<p><a href="https://nbviewer.jupyter.org/github/JaumeClave/scenario_analysis_and_modern_portfolio_theory_mpt_with_efficient_frontiers/blob/master/scenario_analysis_and_modern_portfolio_theory_with_efficient_frontiers.ipynb" target="_blank">nbviewer</a></p>

<a href="https://nbviewer.jupyter.org/github/JaumeClave/scenario_analysis_and_modern_portfolio_theory_mpt_with_efficient_frontiers/blob/master/scenario_analysis_and_modern_portfolio_theory_with_efficient_frontiers.ipynb" target="_blank">
<img src="https://jaumeclave.github.io./images/efficient_frontiers.JPG" width="350" align="right"/>
</a>

<p align="justify">The use of scenarios in strategic decision making was pioneered by the Royal Dutch/Shell Company during the 1970s. The method is best suited to organizations and industries which are extremely sensitive to external factors beyond their control and where time frames are relatively long. In financial risk management, scenario analysis is commonly used to estimate changes to a portfolio's value in response to an unfavorable event and may be used to examine a theoretical worst-case scenario. Prior to Shell’s scenario analysis introduction, in 1952, an economist named Harry Markowitz wrote his dissertation on “Portfolio Selection”, a paper that contained theories which transformed the landscape of portfolio management. Since then the most important problem in investment has always been to construct a portfolio of assets which takes both return and risk into proper consideration. A set of holdings must be selected that are designed to conform to the investment objectives and the risk appetite. Crucially this risk appetite is not just expressed in terms of volatility or value-at-risk, but rather in terms of the portfolio’s performance in all foreseeable future states of the markets. <a href="https://nbviewer.jupyter.org/github/JaumeClave/scenario_analysis_and_modern_portfolio_theory_mpt_with_efficient_frontiers/blob/master/scenario_analysis_and_modern_portfolio_theory_with_efficient_frontiers.ipynb" target="_blank"><b>This project</b></a> runs various scenario analyses using “The Greeks” as a basis of portfolio Profit and Loss estimation. It then produces and derives the optimisation problem behind MPT, using the cvxopt package to solve for various efficient frontiers on monthly asset returns and their respective covariance matrices. Various constraints are declared on the solver to satisfy specific market conditions. Special portfolios risk-averse and risk-seeking investors might desire are identified and visualised on the frontier.</p>


<h3 id="face-recognition-pca">Building a Facial Recognition Algorithm Using Principal Component Analysis (PCA)</h3>
<p><a href="https://nbviewer.jupyter.org/github/JaumeClave/facial_recognition_with_pca/blob/master/face_detection_with_principal_component_analysis.ipynb" target="_blank">nbviewer</a></p>

<a href="https://nbviewer.jupyter.org/github/JaumeClave/facial_recognition_with_pca/blob/master/face_detection_with_principal_component_analysis.ipynb" target="_blank">
<img src="https://jaumeclave.github.io./images/3d_pca.JPG" width="350" align="right"/>
</a>

<p align="justify">Facial recognition is utilized by SnapChat and Instagram to attract a wider user base amidst stiff competition from different applications through filters and animated lenses. Apple and Samsung have patented biometric authentication that utilize facial recognition sensors on the devices front. Facebook has a DeepFace system that employs a nine-layer neural network with over 120 million connection weights, trained on four million images uploaded by Facebook users. DeepFace shows human-level performance even outperforming us in various situations. This technology is outperforms the FBI's Next Generation Identification system. Competing with this is China which is developing facial recognition technology to identify people wearing surgical or dust masks by matching solely to eyes and foreheads. <a href="https://nbviewer.jupyter.org/github/JaumeClave/facial_recognition_with_pca/blob/master/face_detection_with_principal_component_analysis.ipynb" target="_blank"><b>This project</b></a> introduces principal component analysis to build a simple facial recognition model on two unique datasets. The unsupervised machine learning model is explained, visualized and mathematically expressed in order to build intuition. The eigenfaces are plotted from the original 4 faces along with k-dimensional reconstructions of them for k= 3, 10, 25, 50 and 200. Various machine learning models, L/QDA, Support Vector Classifier, Gaussian NB and a Multi-Layer Perceptron are built, tuned and fitted to the faces dataset for classification. Finally a synthetic dataset is created and a K-means clustering model is built. Performance of the model, the quality of the clusters, is compared to when the model is trained on raw data and to when it is trained on principal component vectors.</p>


<h3 id="whatsapp_chatbot_python">	Django, Webhooks and Natural Language Processing for Making a Responsive WhatsApp Chatbot</h3>
<p><a href="https://nbviewer.jupyter.org/github/JaumeClave/python_chatbots/blob/main/python_chatbots.ipynb" target="_blank">nbviewer</a></p>

<a href="https://nbviewer.jupyter.org/github/JaumeClave/python_chatbots/blob/main/python_chatbots.ipynb" target="_blank">
<img src="https://jaumeclave.github.io./images/whatsapp_chatbot_gif.gif" width="350" align="right"/>
</a>

<p align="justify">Facial recognition is utilized by SnapChat and Instagram to attract a wider user base amidst stiff competition from different applications through filters and animated lenses. Apple and Samsung have patented biometric authentication that utilize facial recognition sensors on the devices front. Facebook has a DeepFace system that employs a nine-layer neural network with over 120 million connection weights, trained on four million images uploaded by Facebook users. DeepFace shows human-level performance even outperforming us in various situations. This technology is outperforms the FBI's Next Generation Identification system. Competing with this is China which is developing facial recognition technology to identify people wearing surgical or dust masks by matching solely to eyes and foreheads. <a href="https://nbviewer.jupyter.org/github/JaumeClave/facial_recognition_with_pca/blob/master/face_detection_with_principal_component_analysis.ipynb" target="_blank"><b>This project</b></a> introduces principal component analysis to build a simple facial recognition model on two unique datasets. The unsupervised machine learning model is explained, visualized and mathematically expressed in order to build intuition. The eigenfaces are plotted from the original 4 faces along with k-dimensional reconstructions of them for k= 3, 10, 25, 50 and 200. Various machine learning models, L/QDA, Support Vector Classifier, Gaussian NB and a Multi-Layer Perceptron are built, tuned and fitted to the faces dataset for classification. Finally a synthetic dataset is created and a K-means clustering model is built. Performance of the model, the quality of the clusters, is compared to when the model is trained on raw data and to when it is trained on principal component vectors.</p>


<h3 id="logistic-regression-patient-readmittance">Using Logistic Regression to Predict Hospital Patient Readmittance</h3>
<p><a href="https://nbviewer.jupyter.org/github/JaumeClave/tahoe_healthcare_readmittance_prediction/blob/master/tahoe_healthcare_systems.ipynb" target="_blank">nbviewer</a></p>

<a href="https://nbviewer.jupyter.org/github/JaumeClave/tahoe_healthcare_readmittance_prediction/blob/master/tahoe_healthcare_systems.ipynb" target="_blank">
<img src="https://jaumeclave.github.io./images/roc_curve.JPG" width="350" align="right"/>  </a>

<p align="justify">The high readmission level of hospital patients after they are discharged from the hospital is a significant concern for the US health care system. It is estimated that 20% of all hospitalized Medicare patients are readmitted within 30 days of hospitalization and 34% are readmitted within 90 days. The estimated cost of hospital readmissions is about $17.4 billion annually. To address the problem, the 2010 Affordable Care Act established a hospital readmissions reduction program (HRRP). The program created financial incentives for hospitals to reduce readmissions by linking Medicare reimbursements to a hospital’s risk-adjusted readmission rate. For 2012, penalties could be as much 1% of the total reimbursements a hospital received for the three target conditions. In the first year of the program, 2,225 hospitals were subject to reduced payment penalties, with penalties totaling $225 million nationwide. The maximum penalties were set to increase to 3% of reimbursements by 2014.<a href="https://nbviewer.jupyter.org/github/JaumeClave/tahoe_healthcare_readmittance_prediction/blob/master/tahoe_healthcare_systems.ipynb" target="_blank"><b> This project </b></a> studies the hospitals readmission rate and the costs associated with the Medicare penalties. The paper also considers a CareTracker program that aims to help paitents post care and reduce readmission rates in order to reduce Medicare penalties. A logistic regression model is applied to determine the best possible threshold, based on patient readmittence probabiloty, to apply when deciding to offer the CareTracker program or not.</p>


<h3 id="geometric-brownian-motion-black-sholes-delta-hedging">Geometric Brownian Motion and Black-Scholes Delta Hedging</h3>
<p><a href="https://nbviewer.jupyter.org/github/JaumeClave/Geometric-Brownian-Motion-and-Black-Scholes-Delta-Hedging/blob/master/geometric_brownian_motion_and_black_scholes_delta_hedging.ipynb" target="_blank">nbviewer</a></p>

<a href="https://nbviewer.jupyter.org/github/JaumeClave/Geometric-Brownian-Motion-and-Black-Scholes-Delta-Hedging/blob/master/geometric_brownian_motion_and_black_scholes_delta_hedging.ipynb" target="_blank">
<img src="https://jaumeclave.github.io./images/monte_carlos.JPG" width="350" align="right"/></a>

<p align="justify">Summary statistics released in early January of 2020 by FIA show that trading activity in the global exchange-traded derivatives markets rose by 13.7% in 2019 to reach a record of 34.47 billion contracts. The options volume alone rose 16% to 15.23 billion contracts. Price movements in the options market are a reflection of decisions to buy or sell options made by millions of traders. Successful traders study the options daily trading volume and open interest when on the exchange. <a href="https://nbviewer.jupyter.org/github/JaumeClave/Geometric-Brownian-Motion-and-Black-Scholes-Delta-Hedging/blob/master/geometric_brownian_motion_and_black_scholes_delta_hedging.ipynb" target="_blank"><b>This project</b></a> introduces the options derivative and provides terminology and the mathematical notation behind essential option pricing models. A function is built to calibrate the binomial model so that its dynamics match that of GBM allowing for convergence of binomial model option prices to Black-Scholes prices. A Monte Carlo simulation of 10,000 paths is built to estimate the price of an Asian call option. The project assumes certain market conditions so that it becomes possible to dynamically replicate the payoff of a call option using a self-financing trading strategy. The strategy requires continuous and dynamic delta hedging in the Black-Sholes model which is investigated in order to create profit and loss computations based on option volatility.</p>


<h3 id="networkx-asset-price-correlations">Applying Network Analytics to Visualize ETF Asset Price Correlations</h3>
<p><a href="https://nbviewer.jupyter.org/github/JaumeClave/asset_price_correlation_NetworkX/blob/master/asset_price_correlation.ipynb" target="_blank">nbviewer</a></p>

<a href="https://nbviewer.jupyter.org/github/JaumeClave/asset_price_correlation_NetworkX/blob/master/asset_price_correlation.ipynb" target="_blank">
<img src="https://jaumeclave.github.io./images/clustured_heatmap.JPG" width="350" align="right"/>
</a>

<p align="justify">“Diversify your portfolio!” Words everyone has heard and a portfolio managers priority. Diversifying methods vary from selecting different asset classes (funds, bonds, stocks, etc.), combining industries, or varying the risk levels of investments. And the most common and direct diversification measurement used in these methods is correlation. Correlation is how closely variables are related and it may be measured with Pearsons correlation coefficient, the degree of linear relationship between two variables. Its values range between -1 (perfect negative correlation) and 1 (perfect positive correlation). While a zero correlation implies no relationship between variables. True diversification is therefore only realistically achieved by investing in assets which are uncorrelated (0) with each other. <a href="https://nbviewer.jupyter.org/github/JaumeClave/asset_price_correlation_NetworkX/blob/master/asset_price_correlation.ipynb" target="_blank"><b>This project</b></a> uses NetworkX and nxviz to investigate and visualize these relationships and investigate price correlations for 39 different assets (currencies, commodities, equities and bonds) with the aim of showing an investor what assets they might need to hold to truly diversify their portfolio.</p>


<h3 id="rfm-clv-customer-segmentation">Retail Customer Segmentation Through RFM and Custmer Lifetime Value Analysis</h3>
<p><a href="https://nbviewer.jupyter.org/github/JaumeClave/customer_analysis_FRM_CLV/blob/master/direct_marketing_educational_foundation_customer_analysis.ipynb" target="_blank">nbviewer</a></p>

<a href="https://nbviewer.jupyter.org/github/JaumeClave/customer_analysis_FRM_CLV/blob/master/direct_marketing_educational_foundation_customer_analysis.ipynb" target="_blank"> 
<img src="https://jaumeclave.github.io./images/month_on_month_rev.JPG" width="350" align="right"/> </a>

<p align="justify">The dataset that is studied to discover customer insights and how different marketing channels impacts their purchase behavior has been provided by a multichannel company with sales of several hundred million dollars per year. The company has a network of retail stores, a well-established traditional catalog channel and a website. Its brand is very well known nationally (US) and it has a strong positive reputation along with very good long term customer loyalty. The core of the company’s business consists of food products which are often purchased as gifts during the Christmas season. <a href="https://nbviewer.jupyter.org/github/JaumeClave/customer_analysis_FRM_CLV/blob/master/direct_marketing_educational_foundation_customer_analysis.ipynb" target="_blank"><b>This project</b></a> explores this dataset and visualizes key components such as VIP customer spending, marketing channel conversion rate and order method splits before producing a more detailed section on RFM segmentation and the process involving the calculation for customer lifetime value. The data is initially pushed to a local PostgreSQL database and queired using the SQLAlchemy ORM library and Psycopg2  in order to prepare various tables needed for the investifation. RFM segmentation is a great method to identify groups of customers for special treatment because it allows marketers to target specific clusters of customers with communications that are much more relevant for their particular behavior – and thus generate much higher rates of response, plus increased loyalty and customer lifetime value.</p>


<h3 id="networkx-community-detection">Data Clustering and Community Detection in Graphs Using NetworkX</h3>
<p><a href="https://nbviewer.jupyter.org/github/JaumeClave/community_detection_NetworkX/blob/master/community_detection-networkx.ipynb" target="_blank">nbviewer</a></p>

<a href="https://nbviewer.jupyter.org/github/JaumeClave/community_detection_NetworkX/blob/master/community_detection-networkx.ipynb" target="_blank">
<img src="https://jaumeclave.github.io./images/louvian_wttws.JPG" width="350" align="right"/></a>

<p align="justify">Networks are graphs which are made out of nodes and edges and they are present everywhere. Social networks composed of users owned by the likes of Facebook and Twitter contain sensitive relationship data, biological networks help analyse patterns in biological systems, such as food-webs and predator-prey interactions and narrative networks help identify key actors and the key communities or parties they are involved with. The study of a network is essential in order to learn about its information spread, players of influence and its robustness. Networks inherently contain communities, areas of densely connected nodes which provide information about the network, among that information, it allows for the creation of large scale maps of a network since individual communities act like meta-nodes in the network. NetworkX is a Python package for the creation, manipulation, and study of the structure, dynamics, and functions of complex networks. <a href="https://nbviewer.jupyter.org/github/JaumeClave/community_detection_NetworkX/blob/master/community_detection-networkx.ipynb" target="_blank"><b>This project</b></a> utilises NetwokX to investigate two different networks, studying key centrality measures and utilising the Girvan–Newman and the Louvain Modularity methods to explore network communities.</p>


<h3 id="markov-chain-attribution">Visualizing the Customer Funnel and Using Markov Chains for Website Page Attribution</h3>
<p><a href="https://nbviewer.jupyter.org/github/JaumeClave/website_user_flow_with_markov_chain_attribution/blob/0f914550c22dc960846b7a82bb4fda837f53614b/user_flow_and_markov_chain_attribution.ipynb" target="_blank">nbviewer</a></p>

<a href="https://nbviewer.jupyter.org/github/JaumeClave/website_user_flow_with_markov_chain_attribution/blob/0f914550c22dc960846b7a82bb4fda837f53614b/user_flow_and_markov_chain_attribution.ipynb" target="_blank">
<img src="https://jaumeclave.github.io./images/sankey_diagrams.png" width="350" align="right"/></a>

<p align="justify">Websites collect a vast array of data for many different uses. This includes data you provide via forms, for example, email address and credit card information, as well as many other types of information gained from tracking technology in the form of cookies. Decision makers need to form data-driven approaches to enhance their website framework in order to facilitate user journey and purchasing funnel from entrance to conversion. By investigating click data and website behavior metrics the site may be optimized and user experience and therefore company revenue increased. This project investigates anonymized data from an ecommerce site. It analyzes the data to understand how customers behave and purchase on the site and provides recommendations to management for website restructure. <a href="https://nbviewer.jupyter.org/github/JaumeClave/website_user_flow_with_markov_chain_attribution/blob/0f914550c22dc960846b7a82bb4fda837f53614b/user_flow_and_markov_chain_attribution.ipynb" target="_blank"><b>This project</b></a> uses Markov chains and transition matrices to identify the probabilities of moving from one event (page) to another. It visualizes the user flow through various interactive sankey models that rank and provide information for each step in the conversion funnel. The layout of the website and its pages are presented through a network analytics study in which various graphing algorithms are used to depict the traffic between the directed network.</p>


<h3 id="data-analysis-and-machine-learning-airbnb-new-orleans">Exploring & Machine Learning With Airbnb Listings in New Orleans</h3>
<p><a href="https://nbviewer.jupyter.org/github/JaumeClave/inside_airbnb_new_orleans_analysis/blob/master/inside_airbnb_new_orleans_analysis.ipynb" target="_blank">nbviewer</a></p>

<a href="https://nbviewer.jupyter.org/github/JaumeClave/inside_airbnb_new_orleans_analysis/blob/master/inside_airbnb_new_orleans_analysis.ipynb" target="_blank">
<img src="https://jaumeclave.github.io./images/new_orleans_neigborhoods_sentiment.JPG" width="350" align="right"/>  </a>

<p align="justify"><a href="https://nbviewer.jupyter.org/github/JaumeClave/inside_airbnb_new_orleans_analysis/blob/master/inside_airbnb_new_orleans_analysis.ipynb" target="_blank"><b>This project</b></a> provides an analysis and evaluation of the current and prospective Airbnb listings in New Orleans (NO), Louisiana. The report intends to contribute information and advice to potential visitors. Methods of analysis include sentiment analysis, data and feature aggregation as well as visualizations such as boxplots, spatial mappings and calendar plots. Other calculations include occupancy rate, prices and trip length averages. Results show that occupancy rate is higher during key dates in NO. In particular, during NOs large events such as the BUKU Music and Art Project and the New Orleans Jazz and Heritage Festival. This produces an increment in property price and demand. The report finds that the prospects of Airbnb in the NO community is positive because of high (+80) review scores across all neighbourhoods and property types. The majority of properties are of type “house” with a categorized size of small and medium, ideal for families and groups of friends. Properties in summer months are comparatively available resulting in a peaceful and quite vacation.</p>


<h3 id="forecasting-lettuce-demand-in-fast-food-resturants">Forecasting Ingredient Demand for Fast Food Resturants in New York and California</h3>
<p><a href="https://github.com/JaumeClave/lettuce_forecast/blob/master/README.md" target="_blank">Github</a></p>

<a href="https://github.com/JaumeClave/lettuce_forecast/blob/master/README.md" target="_blank">
<img src="https://jaumeclave.github.io./images/unnameds-chunk-54-1.png" width="350" align="right"/>  </a>

<p align="justify">Forecasting methods are utilised in all industries. The ability to make predictions of the future based on past and present data and most commonly by analysis of trends is key to success in any market because if done successfully it may drastically reduce costs and increase company revenue and profit. The food industry in this sense, is no different. With ever-changing tastes, tight profit margins, and fickle consumers, the need for accurate projections is an essential component of business success. Industry leaders use state of the art forecasting techniques, centralized data sources and machine learning algorithms to get a step ahead of the competition and to optimally manage their supply chain. In general however, the problem the industry faces lies with scattered data sources, insufficient consideration of external factors, such as campaigns and promotions, moving holidays, weather and ad hoc solutions. <a href="https://github.com/JaumeClave/lettuce_forecast/blob/master/README.md" target="_blank"><b>This project</b></a> forecasts the daily demand for ingredients in a fast-food chain in two different major US markets; New York and California. Each restaurant's historic data is examined and the data is checked for stationarity, transformed and forecasted through an ETS model and a Holt-Winters model. An ARIMA model is also used to forecast ingredient demand in order to test various methods. Finally all three models are evaluated against each other using various estimators of prediction errors and the best is chosen for each individual restaurant location.</p>


<h3 id="Marketing_Resource_Allocation_VAR_&_IRF">Marketing Resource Allocation With Vector Autoregression and Impulse Response Analysis</h3>
<p><a href="https://nbviewer.jupyter.org/github/JaumeClave/Marketing_Resource_Allocation_VAR_-_IRF/blob/master/Marketing%20Resource%20Allocation%20%28VAR%20%26%20IRF%29.ipynb" target="_blank">nbviewer</a></p>

<a href="https://nbviewer.jupyter.org/github/JaumeClave/Marketing_Resource_Allocation_VAR_-_IRF/blob/master/Marketing%20Resource%20Allocation%20%28VAR%20%26%20IRF%29.ipynb" target="_blank">
<img src="https://jaumeclave.github.io./images/optimal_budget_allocations.JPG" width="350" align="right"/>  </a>

<p align="justify">The 2019 global advertisement budget surpassed $560 billion USD, the US and North-American market, unsurpsignly being the largest spenders. These budgets are stretched among various channels as internet advertising now not only relates to “traditional” Google Ads spending or Facebook Ads spending but to influencer marketing on quickly growing social media apps like Tik-Tok and Snapchat. As new marketing opportunities arise decision makers not only need to figure out how much company resource to devote to their marketing campaigns, they need to decide where exactly to spend that budget. The old axiom, “marketers waste half of their money, but they just don’t know which half” still holds true. <a href="https://nbviewer.jupyter.org/github/JaumeClave/Marketing_Resource_Allocation_VAR_-_IRF/blob/master/Marketing%20Resource%20Allocation%20%28VAR%20%26%20IRF%29.ipynb" target="_blank"><b>This project</b></a> contains a detailed analytical investigation into a retailers marketing budget and it provides an improvement to it by finding its optimal channel allocation. The examination is completed to help the retailer better use their annual marketing budget in order to increase audience reach, conversions and ultimately sales that lead to bottom-line growth. The paper will take a step-by-step approach in solving the optimal resource (channel) allocation problem and will show and explain at each phase what needs to be done to find optimality so that other retailers and companies may adopt the same approach to better allocate their budget.</p>


## About
<p align="justify">I am a self-motivated, enthusiastic team worker with a keen interest in Machine Learning and Data Science. I am passionate about solving business problems through data storytelling and helping make jobs better and more efficient by understanding and supporting the organizational direction. I enjoy being strategic with data and realizing what needs to be acquired to increase the value of the existing data.</p>
 
<p align="justify">My understanding of the importance of data evolved during my time in the digital marketing industry where my responsibility was to measure and track the customer journey touchpoints involved in an online event. My MSc in Business Analytics at Imperial College Business School developed my statistical, operations research and machine learning techniques aimed at solving business problems and obtaining actionable business insight from data. My current work as a Data Scientist in the FinTech and Oil & Energy SaaS industries has allowed me to explore how DS and ML methodologies can be leveraged to add significant business value by analyzing, mining and exploiting available information.</p>

<p align="justify">I enjoy working directly with clients and establishing meaningful and transparent relationships in order to foster mutual growth. I love learning about what makes an individual business succeed and its business model in order to see where I can help. I look forward to connecting with you on <a href="https://www.linkedin.com/in/jaume-clave-domenech/" target="_blank">LinkedIn</a>, you can email me at <a href="mailto:j.clavedomenech@gmail.com" target="_blank">j.clavedomenech@gmail.com</a> and you can explore my projects here on my <a href="https://jaumeclave.github.io./" target="_blank"> website</a>.</p>

<p align="justify">TL;DR: I'm extremely interested in implementing AI, Data Science and Machine Learning to approach difficult problems and uncover patterns in data. Helping leaders understand what questions they are not asking, but should be asking! 🔎 📈  </p>


## Hackathons
<h3 id="AI-Hack-2020">AI Hack 2020 (1st Prize)</h3>
<img src="https://jaumeclave.github.io./images/ai_hacks_2020.jpg" width="125" align="right"/>
<p align="justify">During the last weekend on February I took part in the largest student-led AI Hackathon in the UK. This third ever AI Hack aimed to bring the most forward-thinking and creative student data scientists to solve some of the world's most pressing challenges. I tackled the 24-hour hackathon at Imperial College London and was awarded first prize for the discoveries in the Airbnb Correlation One Challenge! Analysed over 40,000 Airbnb listings in New York City to understand how a listings features affected its nightly price. Explored patterns in the rentals industry and how the they relate to economic, demographic, and geographic trends at large. Created machine learning models that were able to predict a monthly revenue investors could expect from their Airbnb property. This was then used to compare yields from other property rental types so that a potential investor could make a data-driven decision on what to invest in. </p>


<h3 id="AI-Hack-2020">COVID-19 Recovery Challange</h3>
<img src="https://jaumeclave.github.io./images/covid_19_recovery.jpg" width="125" align="right"/>
<p align="justify">Took part in the 5-day innovation hackathon for all bright minds from LSE, Imperial College, Harvard, UCL and ITESM who wanted to help build a better future during these times. The aim of the event was to facilitate the generation of innovative solutions to the challenges engendered by the pandemic by providing 5 days of industry-leading talks, mentorship, networking, guidance all through an online community. This was not a traditional tech hackathon but one which gave the freedom to come up with new products, business models, policies or campaigns trying to help individuals and businesses recover post the crisis.</p>


## Certifications
<h3 id="DataCamp-Data-Scientist-Career-Track">Data Scientist with Python</h3>
<img src="https://jaumeclave.github.io./images/Data_Scientist.png" width="125" align="right"/>
<p align="justify">This 23 course and 88 hour DataCamp career track helped me reinforce my importing, cleaning, manipulating, and visualizing data—all integral skills needed in the data science world. Through interactive exercises and 6 hands-on projects I took advantage of many Python libraries, including pandas, NumPy, Matplotlib, Seaborn, scikit-learn and Keras. I worked with real-world datasets, counting a Super Bowl ad dataset and a Nobel Laureates database to learn the statistical and machine learning techniques needed to train decision trees, support vector machines and artificial neural networks and use natural language processing (NLP) along with image categorization. <a href="https://github.com/JaumeClave/JaumeClave.github.io/blob/master/certifications/data_scientist_career_track_certificate.pdf" target="_blank">This course</a> has helped me grow my Python skills, and has helped me become a more confident data scientist by providing me the skills to complete the <a href="https://jaumeclave.github.io./#projects">projects</a> in my portfolio.</p>


<h3 id="DataCamp-Machine-Learning-Career-Track">Machine Learning Scientist with Python</h3>
<img src="https://jaumeclave.github.io./images/Machine_Learning_Scientist.png" width="125" align="right"/>
<p align="justify">This 23 course and 93 hour DataCamp career track helped me augment my Python programming skill set with the toolbox to perform supervised, unsupervised, and deep learning as a machine learning scientist. I mastered how to process data for features, train models, assess performance, and tune parameters for better performance. The course contained various natural language processing, image processing projects, completed with popular libraries such as TensorFlow, Spark and Keras. <a href="https://github.com/JaumeClave/JaumeClave.github.io/blob/master/certifications/machine_learning_scientist_career_track_certificate.pdf" target="_blank">This course</a> has helped me improve my machine learning skills, and has helped me become a more confident programmer by providing me the tools needed to complete the <a href="https://jaumeclave.github.io./#projects">projects</a> in my portfolio.</p>


<h3 id="DataCamp-Machine-Learning-Fundamentals">Machine Learning Fundamentals with Python</h3>
<img src="https://jaumeclave.github.io./images/Machine_Learning_Fundamentals.png" width="125" align="right"/>
<p align="justify">This 5 course and 20 hour DataCamp skill track helped me practise the fundamental concepts in Machine Learning. scikit-learn was used throughout this certification to build predictive models, tune their parameters, and determine how well they will perform with unseen data. SciPy was levereaged to cluster, transform, visualize, and extract insights from unlabeled datasets, and build a recommender system to recommend popular musical artists to users. A school budgeting case study was used to build a baseline model before preforming natural language processing to prepare the budgets for modeling. Deep learning with the Keras 2.0 library was implemented in order build deep neural networds and optimize them with backward propagation and keras fine-tunning. <a href="https://github.com/JaumeClave/JaumeClave.github.io/blob/master/certifications/machine_learning_fundamentals_skill_track_certificate.pdf" target="_blank">This course</a> has helped me grow my Python and Machine Learning skills, and has helped me become a more confident data scientist by providing me the skills to complete the <a href="https://jaumeclave.github.io./#projects">projects</a> in my portfolio.</p>


## Papers
<h3 id="Client-Churn-Prediction">Assessing Customer Churn Using Machine Learning Models in the Oil & Energy SaaS Industry</h3>
<p align="justify"> Supervised by Dr. W. Wiesemann. Examined the client renewal process for Aucerna and provided a probabilistic classifier that output probabilities based on a client not renewing its contract for its original amount. Identified active customer churn which is detectable by analysing hidden patterns in customer account data, payment behaviour and history data, product and solution usage data and the business-to-customer communication. Enhanced the risk assesment process by introducing a data-driven churn score. Collaborations with Aucerna.</p>

<h3 id="Economic-Analysis-Hydrualic-Fracturing">Economic Analysis on the Profitability of a Horizontal Multi-Stage Hydraulic Fracturing well in the Duvernay Formation</h3>
<p align="justify"> Supervised by Dr. D. Stupples. Project titled: ‘Economic Analysis on the Profitability of a Horizontal Multi-Stage Hydraulic Fracturing well in the Duvernay Formation’. Examined the economic viability of a 20-year lifespan petroleum extraction site in Alberta, Canada by investigating potential production of shale oil deposits. Oil production data analysed and forecasted using MATLAB and Simulink. Collaborations with 3ESI-Enersight.</p>


<a href="https://jaumeclave.github.io./#">Back to Top</a>


<details><summary></summary>
 <!-- Google Tag Manager -->
<script>(function(w,d,s,l,i){w[l]=w[l]||[];w[l].push({'gtm.start':
new Date().getTime(),event:'gtm.js'});var f=d.getElementsByTagName(s)[0],
j=d.createElement(s),dl=l!='dataLayer'?'&l='+l:'';j.async=true;j.src=
'https://www.googletagmanager.com/gtm.js?id='+i+dl;f.parentNode.insertBefore(j,f);
})(window,document,'script','dataLayer','GTM-MLPJCVN');</script>
<!-- End Google Tag Manager -->
<!-- Google Tag Manager (noscript) -->
<noscript><iframe src="https://www.googletagmanager.com/ns.html?id=GTM-MLPJCVN"
height="0" width="0" style="display:none;visibility:hidden"></iframe></noscript>
<!-- End Google Tag Manager (noscript) -->
<p>Nothing to see here!</p>
</details>
